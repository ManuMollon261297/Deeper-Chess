{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SgZ4RfvsNqrk",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load Libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "kp1Ta-pi46NT",
    "outputId": "56aa46f5-1699-4003-eed9-88c236394027"
   },
   "outputs": [],
   "source": [
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "def cpu():\n",
    "  with tf.device('/cpu:0'):\n",
    "    random_image_cpu = tf.random.normal((100, 100, 100, 3))\n",
    "    net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)\n",
    "    return tf.math.reduce_sum(net_cpu)\n",
    "\n",
    "def gpu():\n",
    "  with tf.device('/device:GPU:0'):\n",
    "    random_image_gpu = tf.random.normal((100, 100, 100, 3))\n",
    "    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\n",
    "    return tf.math.reduce_sum(net_gpu)\n",
    "# We run each op once to warm up; see: https://stackoverflow.com/a/45067900\n",
    "cpu()\n",
    "gpu()\n",
    "\n",
    "# Run the op several times.\n",
    "print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '\n",
    "      '(batch x height x width x channel). Sum of ten runs.')\n",
    "print('CPU (s):')\n",
    "cpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\n",
    "print(cpu_time)\n",
    "print('GPU (s):')\n",
    "gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\n",
    "print(gpu_time)\n",
    "print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainDBN(layers,xTrain):\n",
    "  pop = 1\n",
    "  push = 0\n",
    "  model = tf.keras.Sequential()\n",
    "  opt = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "  for i in range(len(layers)-1):\n",
    "    auxLayers = list()\n",
    "    if push != 0:\n",
    "      # Mark layers as non-trainable\n",
    "      for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "      for j in range(pop):\n",
    "        # remember the current output layer\n",
    "        auxLayers.append(model.layers[-1])\n",
    "        # remove the output layer\n",
    "        for layer in model.layers:\n",
    "          print(layer)\n",
    "        model.pop()\n",
    "    # Generate new hidden layers\n",
    "    model.add(tf.keras.layers.Dense(units=layers[i], activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(units=layers[i+1], activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(units=layers[i], activation='relu'))\n",
    "\n",
    "#    for layer in auxLayers:\n",
    "#      print(layer.name)\n",
    "#      print(\"Weights\")\n",
    "#      print(\"Shape: \",layer.get_weights()[0].shape,'\\n',layer.get_weights()[0])\n",
    "\n",
    "    # push\n",
    "    if push != 0:\n",
    "      # push saved layers\n",
    "      for k in range(push):\n",
    "        model.add(auxLayers[push-1-k])\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=opt,\n",
    "                loss='mean_squared_error',\n",
    "                metrics=['accuracy'])\n",
    "    # Train NN\n",
    "    num_epochs = 200\n",
    "    batch_size = 256\n",
    "    history = model.fit(x=xTrain, y=xTrain,\n",
    "                        epochs=num_epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=True,\n",
    "                        validation_data=(xTrain, xTrain),\n",
    "                        verbose=1)\n",
    "    pop += 1\n",
    "    push += 1\n",
    "    model.summary()\n",
    "    # Fix possible problems with new model\n",
    "    model.save('temp.h5')\n",
    "    model = load_model('temp.h5')\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RjxJpyftEW25"
   },
   "outputs": [],
   "source": [
    "# Separate Data in arrays\n",
    "f1 = open('bitStreamPosDataFinalB.txt')\n",
    "f2 = open('bitStreamPosDataFinalW.txt')\n",
    "gamesArray = list()\n",
    "for line in f1:\n",
    "  gamesArray1 = line.split()\n",
    "for line in f2:\n",
    "  gamesArray2 = line.split()\n",
    "gamesArray = gamesArray1 + gamesArray2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CRf8OLksdgrD"
   },
   "outputs": [],
   "source": [
    "f1.close()\n",
    "f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "FfHd3KUJIS1u",
    "outputId": "76bff02f-06e3-4c67-d863-d352f7b16d03"
   },
   "outputs": [],
   "source": [
    "print(len(gamesArray))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain = list()\n",
    "for string in gamesArray:\n",
    "  auxGame = np.zeros(773, dtype=np.uint8) \n",
    "  j = 0\n",
    "  for bit in string:\n",
    "    auxGame[j] = bit\n",
    "    j += 1\n",
    "  xTrain.append(auxGame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(xTrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(xTrain[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain = xTrain[:2000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(xTrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nZyvCiD4fwAH"
   },
   "outputs": [],
   "source": [
    "model = trainDBN([773,600,400,200,100],xTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Do1CpXxV99d3"
   },
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/unsupervised-machine-learning-example-in-keras-8c8bf9e63ee0\n",
    "\n",
    "def anomalyScores(originalDF, reducedDF):\n",
    "    loss = np.sum((np.array(originalDF) - \\\n",
    "                   np.array(reducedDF))**2, axis=1)\n",
    "    loss = pd.Series(data=loss,index=originalDF.index)\n",
    "    loss = (loss-np.min(loss))/(np.max(loss)-np.min(loss))\n",
    "    \n",
    "    print('Mean for anomaly scores: ', np.mean(loss))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kHKlWksY9_LD"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(xTrain, verbose = 1)\n",
    "anomalyScore = anomalyScores(xTrain, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ygkRis-76S5C"
   },
   "outputs": [],
   "source": [
    "def trimAutoencoder(model):\n",
    "  for i in range(int(len(model.layers)/2)):\n",
    "    model.pop()\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 318
    },
    "colab_type": "code",
    "id": "nTUkwIhZ9q5Y",
    "outputId": "4b87102e-c4b3-48ff-9d11-3e89ff2a7ab2"
   },
   "outputs": [],
   "source": [
    "model = trimAutoencoder(model)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 318
    },
    "colab_type": "code",
    "id": "8E1kCYDB9_-9",
    "outputId": "5ea075ef-a5dd-4385-babc-a8488bb9c75e"
   },
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "  layer.trainable = True\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "colab_type": "code",
    "id": "cG3qK6Tx-gT6",
    "outputId": "13987056-1e0d-419e-ca3c-9c3f4c1d9662"
   },
   "outputs": [],
   "source": [
    "model.save('UDBNweights.h5')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DBNunsup.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
